<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>Getting started • imagefluency</title>
<!-- favicons --><link rel="icon" type="image/png" sizes="16x16" href="../favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="../favicon-32x32.png">
<link rel="apple-touch-icon" type="image/png" sizes="180x180" href="../apple-touch-icon.png">
<link rel="apple-touch-icon" type="image/png" sizes="120x120" href="../apple-touch-icon-120x120.png">
<link rel="apple-touch-icon" type="image/png" sizes="76x76" href="../apple-touch-icon-76x76.png">
<link rel="apple-touch-icon" type="image/png" sizes="60x60" href="../apple-touch-icon-60x60.png">
<!-- jquery --><script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js" integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin="anonymous"></script><!-- Bootstrap --><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.4.1/css/bootstrap.min.css" integrity="sha256-bZLfwXAP04zRMK2BjiO8iu9pf4FbLqX6zitd+tIvLhE=" crossorigin="anonymous">
<script src="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.4.1/js/bootstrap.min.js" integrity="sha256-nuL8/2cJ5NDSSwnKD8VqreErSWHtnEP9E7AySL+1ev4=" crossorigin="anonymous"></script><!-- bootstrap-toc --><link rel="stylesheet" href="../bootstrap-toc.css">
<script src="../bootstrap-toc.js"></script><!-- Font Awesome icons --><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/all.min.css" integrity="sha256-mmgLkCYLUQbXn0B1SRqzHar6dCnv9oZFPEC1g1cwlkk=" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/v4-shims.min.css" integrity="sha256-wZjR52fzng1pJHwx4aV2AO3yyTOXrcDW7jBpJtTwVxw=" crossorigin="anonymous">
<!-- clipboard.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><!-- headroom.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/headroom.min.js" integrity="sha256-AsUX4SJE1+yuDu5+mAVzJbuYNPHj/WroHuZ8Ir/CkE0=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/jQuery.headroom.min.js" integrity="sha256-ZX/yNShbjqsohH1k95liqY9Gd8uOiE1S4vZc+9KQ1K4=" crossorigin="anonymous"></script><!-- pkgdown --><link href="../pkgdown.css" rel="stylesheet">
<script src="../pkgdown.js"></script><meta property="og:title" content="Getting started">
<meta property="og:description" content="Getting started with the imagefluency package.">
<meta property="og:image" content="https://stm.github.io/imagefluency/logo.png">
<!-- mathjax --><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha256-nvJJv9wWKEm88qvoQl9ekL2J+k/RWIsaSScxxlsrv8k=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/config/TeX-AMS-MML_HTMLorMML.js" integrity="sha256-84DKXVJXs0/F8OTMzX4UR909+jtl4G7SPypPavF+GfA=" crossorigin="anonymous"></script><!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
<![endif]-->
</head>
<body data-spy="scroll" data-target="#toc">
    

    <div class="container template-article">
      <header><div class="navbar navbar-default navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <span class="navbar-brand">
        <a class="navbar-link" href="../index.html">imagefluency</a>
        <span class="version label label-default" data-toggle="tooltip" data-placement="bottom" title="">0.2.4</span>
      </span>
    </div>

    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
<li>
  <a href="../index.html">
    <span class="fas fa-home fa-lg"></span>
     
  </a>
</li>
<li>
  <a href="../articles/getting-started.html">Getting started</a>
</li>
<li>
  <a href="../articles/batch-processing.html">Analyzing multiple images</a>
</li>
<li>
  <a href="../reference/index.html">Reference</a>
</li>
<li>
  <a href="../news/index.html">Changelog</a>
</li>
      </ul>
<ul class="nav navbar-nav navbar-right">
<li>
  <a href="https://github.com/stm/imagefluency/" class="external-link">
    <span class="fab fa-github fa-lg"></span>
     
  </a>
</li>
      </ul>
</div>
<!--/.nav-collapse -->
  </div>
<!--/.container -->
</div>
<!--/.navbar -->

      

      </header><div class="row">
  <div class="col-md-9 contents">
    <div class="page-header toc-ignore">
      <h1 data-toc-skip>Getting started</h1>
            <h3 data-toc-skip class="subtitle">imagefluency: Image Statistics Based on Processing Fluency</h3>
            
            <h4 data-toc-skip class="date">August 25, 2022</h4>
      
      <small class="dont-index">Source: <a href="https://github.com/stm/imagefluency/blob/HEAD/vignettes/getting-started.Rmd" class="external-link"><code>vignettes/getting-started.Rmd</code></a></small>
      <div class="hidden name"><code>getting-started.Rmd</code></div>

    </div>

    
    
<style>
figure {
    display: inline-block;
    horizontal-align: center;
    margin: 2px;
}
figure img {
    vertical-align: bottom;
}
figure figcaption {
    text-align: center;
}
</style>
<div class="section level2">
<h2 id="introduction">Introduction<a class="anchor" aria-label="anchor" href="#introduction"></a>
</h2>
<div class="section level3">
<h3 id="motivation-why-create-yet-another-r-package">Motivation: Why create yet another R package<a class="anchor" aria-label="anchor" href="#motivation-why-create-yet-another-r-package"></a>
</h3>
<p>Over the last decades, the amount of data generated is growing rapidly, predominantly due to digitalization. Most of today’s data is unstructured, and this share is increasing. Given that unstructured data is rich, these data could provide rich insights for scientific research from a variety of fields and practice alike. Especially images (i.e., visual stimuli) are recognized as valuable source of information.</p>
<p>At the same time, vision research and research in psychology shows that even simple changes in low-level image features (like symmetry, contrast, or complexity) can have a tremendous effect on a variety of human judgments. As an example, a statement like “Nut bread is healthier than potato bread” is more likely to be perceived as true when presented in a color that is easy to read against a white background (high contrast) instead of being presented in a color that is difficult to read against a white background (low contrast; cf. <span class="citation">[<a href="#ref-Hansen2008" role="doc-biblioref">1</a>]</span>). Thus, it might be useful to estimate and control for differences in such low-level visual features in any research that includes visual stimuli.</p>
<p><strong>imagefluency</strong> is an simple <code>R</code> package for such low-level image scores based on processing fluency theory. The package allows to get scores for several basic aesthetic principles that facilitate fluent cognitive processing of images: contrast, complexity / simplicity, self-similarity, symmetry, and typicality.</p>
</div>
<div class="section level3">
<h3 id="why-and-when-to-use-the-package">Why and when to use the package<a class="anchor" aria-label="anchor" href="#why-and-when-to-use-the-package"></a>
</h3>
<p>Possible applications include:</p>
<ul>
<li>stimulus selection in experiments (e.g., testing brand logos, evaluate product designs, online display ads)</li>
<li>as control variables in statistical or prediction models</li>
<li>linking image fluency scores to outcomes of interest (e.g., how should a typical product packaging look like, do simpler images get more or less attention on a website, …)</li>
<li>(interpretable) image features in simple machine learning models, e.g. SVM image classifier</li>
</ul>
</div>
</div>
<div class="section level2">
<h2 id="theoretical-background">Theoretical background<a class="anchor" aria-label="anchor" href="#theoretical-background"></a>
</h2>
<p>The most prevailing explanation for how low-level image features affect human judgments is based on processing fluency theory <span class="citation">[<a href="#ref-Reber2004" role="doc-biblioref">2</a>]</span>. Processing fluency describes the ease of processing a stimulus <span class="citation">[<a href="#ref-Schwarz2004" role="doc-biblioref">3</a>]</span>, which happens instantaneously and automatically <span class="citation">[<a href="#ref-Graf2015" role="doc-biblioref">4</a>]</span>. Higher processing fluency results in a gut-level positive affective response <span class="citation">[<a href="#ref-Winkielman2001" role="doc-biblioref">5</a>]</span>. Notably, a rich body of literature has shown that processing fluency effects have an impact on a variety of judgmental domains in our everyday life, including how much we like things, how much we consider statements to be true, how trustworthy we judge a person, how risky we think something is, or whether we buy a product or not (for a review, see <span class="citation">[<a href="#ref-Alter2009" role="doc-biblioref">6</a>]</span>).</p>
<p>Several stimulus features have been proposed that result in increased fluency. In particular, visual symmetry, simplicity, (proto-)typicality, and contrast were identified to facilitate processing <span class="citation">[<a href="#ref-Reber2004" role="doc-biblioref">2</a>]</span>. Recent studies further discuss self-similarity in light of fluency-based aesthetics <span class="citation">[<a href="#ref-Joye2016" role="doc-biblioref">7</a>, <a href="#ref-MayerPACA2018" role="doc-biblioref">8</a>]</span>, a concept which has been studied for example in images of natural scenes <span class="citation">[<a href="#ref-Simoncelli2003" role="doc-biblioref">9</a>]</span>. Self-similarity can be described as self-repeating patterns within a stimulus. A typical example are the leaves of ferns that feature the same shape regardless of any magnification or reduction (i.e., scale invariance). Another prominent example is romanesco broccoli with its self-similar surface.</p>
<p>Extracting image features for contrast, self-similarity, simplicity, symmetry, and typicality therefore constitute the core purpose of the <code>imagefluency</code> package.</p>
</div>
<div class="section level2">
<h2 id="package-overview">Package overview<a class="anchor" aria-label="anchor" href="#package-overview"></a>
</h2>
<div class="section level3">
<h3 id="main-functions">Main functions<a class="anchor" aria-label="anchor" href="#main-functions"></a>
</h3>
<ul>
<li>
<code><a href="../reference/img_contrast.html">img_contrast()</a></code> visual contrast of an image</li>
<li>
<code><a href="../reference/img_complexity.html">img_complexity()</a></code> visual complexity of an image (opposite of simplicity)</li>
<li>
<code><a href="../reference/img_self_similarity.html">img_self_similarity()</a></code> visual self-similarity of an image</li>
<li>
<code><a href="../reference/img_simplicity.html">img_simplicity()</a></code> visual simplicity of an image (opposite of complexity)</li>
<li>
<code><a href="../reference/img_symmetry.html">img_symmetry()</a></code> vertical and horizontal symmetry of an image</li>
<li>
<code><a href="../reference/img_typicality.html">img_typicality()</a></code> visual typicality of a list of images relative to each other</li>
</ul>
</div>
<div class="section level3">
<h3 id="other-helpful-functions">Other helpful functions<a class="anchor" aria-label="anchor" href="#other-helpful-functions"></a>
</h3>
<ul>
<li>
<code><a href="../reference/img_read.html">img_read()</a></code> reads bitmap images into R</li>
<li>
<code><a href="../reference/rgb2gray.html">rgb2gray()</a></code> converts images from RGB into grayscale (might speed up computation)</li>
<li>
<code><a href="../reference/run_imagefluency.html">run_imagefluency()</a></code> launches a (preliminary) Shiny app for an interactive demo of the main functions (alternatively, visit the online version at <a href="https://mayer.shinyapps.io/imagefluency/" class="external-link">shinyapps.io</a>)</li>
</ul>
</div>
<div class="section level3">
<h3 id="installation">Installation<a class="anchor" aria-label="anchor" href="#installation"></a>
</h3>
<p>You can install the current stable version from CRAN.</p>
<div class="sourceCode" id="cb1"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/utils/install.packages.html" class="external-link">install.packages</a></span><span class="op">(</span><span class="st">'imagefluency'</span><span class="op">)</span></span></code></pre></div>
<p>To download the latest development version from Github use the <code>install_github</code> function of the <code>remotes</code> package.</p>
<div class="sourceCode" id="cb2"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># install remotes if necessary</span></span>
<span><span class="kw">if</span> <span class="op">(</span><span class="op">!</span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">require</a></span><span class="op">(</span><span class="st"><a href="https://remotes.r-lib.org" class="external-link">'remotes'</a></span><span class="op">)</span><span class="op">)</span> <span class="fu"><a href="https://rdrr.io/r/utils/install.packages.html" class="external-link">install.packages</a></span><span class="op">(</span><span class="st">'remotes'</span><span class="op">)</span></span>
<span><span class="co"># install imagefluency from github</span></span>
<span><span class="fu">remotes</span><span class="fu">::</span><span class="fu">install_github</span><span class="op">(</span><span class="st">'stm/imagefluency'</span><span class="op">)</span></span></code></pre></div>
<p>After installation, the <code>imagefluency</code> package is loaded the usual way by calling <code><a href="https://stm.github.io/imagefluency/">library(imagefluency)</a></code>. The <code><a href="../reference/img_read.html">img_read()</a></code> function can be used to read an image into R. Just like with reading in a dataset, <code><a href="../reference/img_read.html">img_read()</a></code> expects the path to the file as input, e.g., <code>img_read('C:/Users/myname/Documents/myimage.jpg')</code>. Currently supported file formats are bmp, jpg, png, and tif.</p>
<p>Use the following link to report bugs/issues: <a href="https://github.com/stm/imagefluency/issues" class="external-link uri">https://github.com/stm/imagefluency/issues</a></p>
</div>
</div>
<div class="section level2">
<h2 id="using-imagefluency">Using imagefluency<a class="anchor" aria-label="anchor" href="#using-imagefluency"></a>
</h2>
<p><code>imagefluency</code> allows to get scores for five image features that facilitate fluent processing of images: contrast, complexity / simplicity, self-similarity, symmetry, and typicality.</p>
<p>To use the imagefluency package, first load the library.</p>
<div class="sourceCode" id="cb3"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="va"><a href="https://stm.github.io/imagefluency/">imagefluency</a></span><span class="op">)</span></span></code></pre></div>
<div class="section level3">
<h3 id="contrast">Contrast<a class="anchor" aria-label="anchor" href="#contrast"></a>
</h3>
<p>The function <code><a href="../reference/img_contrast.html">img_contrast()</a></code> returns the contrast of an image. Most research defines contrast in images as the root-mean-squared (RMS) contrast which is the standard deviation of the normalized pixel intensity values <span class="citation">[<a href="#ref-Peli1990" role="doc-biblioref">10</a>]</span>: <span class="math inline">\(\sqrt{\frac{1}{M N}\sum_{i=0}^{N-1}\sum_{j=0}^{M - 1}(I_{ij} - \bar{I})^2}\)</span>. The RMS of an image as a measure for visual contrast has been shown to predict human contrast detection thresholds well <span class="citation">[<a href="#ref-Frazor2006" role="doc-biblioref">11</a>]</span>. Therefore, the function calculates contrast by computing the RMS contrast of the input image. Consequently, a higher value indicates higher contrast. The image is normalized if necessary (i.e., normalization into range [0, 1]). For color images, a weighted average between color the channels is computed (cf. <span class="citation">[<a href="#ref-MayerPACA2018" role="doc-biblioref">8</a>]</span>).</p>
<p>Note that in the following, example images that come with the package are used. Moreover, the images can be displayed using the <code>grid.raster()</code> function from the <code>grid</code> package.</p>
<div class="sourceCode" id="cb4"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Example image with relatively high contrast: berries</span></span>
<span><span class="va">berries</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/img_read.html">img_read</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/system.file.html" class="external-link">system.file</a></span><span class="op">(</span><span class="st">'example_images'</span>, <span class="st">'berries.jpg'</span>, package <span class="op">=</span> <span class="st">'imagefluency'</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="co"># display image</span></span>
<span><span class="fu">grid</span><span class="fu">::</span><span class="fu"><a href="https://rdrr.io/r/grid/grid.raster.html" class="external-link">grid.raster</a></span><span class="op">(</span><span class="va">berries</span><span class="op">)</span></span>
<span><span class="co"># get contrast</span></span>
<span><span class="fu"><a href="../reference/img_contrast.html">img_contrast</a></span><span class="op">(</span><span class="va">berries</span><span class="op">)</span></span></code></pre></div>
<div class="sourceCode" id="cb5"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Example image with relatively low contrast: bike</span></span>
<span><span class="va">bike</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/img_read.html">img_read</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/system.file.html" class="external-link">system.file</a></span><span class="op">(</span><span class="st">'example_images'</span>, <span class="st">'bike.jpg'</span>, package <span class="op">=</span> <span class="st">'imagefluency'</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="co"># display image</span></span>
<span><span class="fu">grid</span><span class="fu">::</span><span class="fu"><a href="https://rdrr.io/r/grid/grid.raster.html" class="external-link">grid.raster</a></span><span class="op">(</span><span class="va">bike</span><span class="op">)</span></span>
<span><span class="co"># get contrast</span></span>
<span><span class="fu"><a href="../reference/img_contrast.html">img_contrast</a></span><span class="op">(</span><span class="va">bike</span><span class="op">)</span></span></code></pre></div>
<p>Calculating the contrast scores for the two images gives the following result:</p>
<center>
<figure><img src="../inst/example_images/bike.jpg"><figcaption>
score: 0.08
</figcaption></figure><figure><img src="../inst/example_images/berries.jpg"><figcaption>
score: 0.29
</figcaption></figure>
</center>
</div>
<div class="section level3">
<h3 id="complexity-simplicity">Complexity / Simplicity<a class="anchor" aria-label="anchor" href="#complexity-simplicity"></a>
</h3>
<p>The function <code><a href="../reference/img_complexity.html">img_complexity()</a></code> returns the visual complexity of an image. Algorithmic information theory indicates that picture complexity can be measured accurately by image compression rates because complex images are denser and have fewer redundancies <span class="citation">[<a href="#ref-Donderi2006" role="doc-biblioref">12</a>, <a href="#ref-Landwehr2011" role="doc-biblioref">13</a>]</span>. Therefore, the function calculates the visual complexity of an image as the ratio between the compressed and uncompressed image file size. Thus, the value does not depend on image size.</p>
<p>The function takes the file path of an image file (or URL) or a pre-loaded image as input argument and returns the ratio of the compressed divided by the uncompressed image file size. The complexity values are naturally interpretable and can range between almost 0 (virtually completely compressed image, thus extremely simple image) and 1 (no compression possible, thus extremely complex image). The function offers to use different image compression algorithms like <code>jpg</code>, <code>gif</code>, or <code>png</code> with <code>algorithm = 'zip'</code> as default (for a discussion about the different algorithms, see <span class="citation">[<a href="#ref-MayerPACA2018" role="doc-biblioref">8</a>]</span>).</p>
<p>As most compression algorithms do not depict horizontal and vertical redundancies equally, the function includes an optional <code>rotate</code> parameter (default: <code>FALSE</code>). Setting this parameter to <code>TRUE</code> additionally creates a compressed version of the <em>rotated</em> image. The overall compressed image’s file size is computed as the minimum of the original image’s file size and the file size of the rotated image.</p>
<p>The function <code><a href="../reference/img_simplicity.html">img_simplicity()</a></code> returns the visual simplicity of an image. Image simplicity is the complement to image complexity and therefore calculated as 1 minus the complexity score (i.e., the compression rate). Values can range between 0 (no compression possible, thus extremely complex image) and almost 1 (virtually completely compressed image, thus extremely simple image).</p>
<div class="sourceCode" id="cb6"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Example image with high complexity: trees</span></span>
<span><span class="va">trees</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/img_read.html">img_read</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/system.file.html" class="external-link">system.file</a></span><span class="op">(</span><span class="st">'example_images'</span>, <span class="st">'trees.jpg'</span>, package <span class="op">=</span> <span class="st">'imagefluency'</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="co"># display image</span></span>
<span><span class="fu">grid</span><span class="fu">::</span><span class="fu"><a href="https://rdrr.io/r/grid/grid.raster.html" class="external-link">grid.raster</a></span><span class="op">(</span><span class="va">trees</span><span class="op">)</span></span>
<span><span class="co"># get complexity</span></span>
<span><span class="fu"><a href="../reference/img_complexity.html">img_complexity</a></span><span class="op">(</span><span class="va">trees</span><span class="op">)</span></span></code></pre></div>
<div class="sourceCode" id="cb7"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Example image with low complexity: sky</span></span>
<span><span class="va">sky</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/img_read.html">img_read</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/system.file.html" class="external-link">system.file</a></span><span class="op">(</span><span class="st">'example_images'</span>, <span class="st">'sky.jpg'</span>, package <span class="op">=</span> <span class="st">'imagefluency'</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="co"># display image</span></span>
<span><span class="fu">grid</span><span class="fu">::</span><span class="fu"><a href="https://rdrr.io/r/grid/grid.raster.html" class="external-link">grid.raster</a></span><span class="op">(</span><span class="va">sky</span><span class="op">)</span></span>
<span><span class="co"># get complexity</span></span>
<span><span class="fu"><a href="../reference/img_complexity.html">img_complexity</a></span><span class="op">(</span><span class="va">sky</span><span class="op">)</span></span></code></pre></div>
<p>Calculating the complexity scores for the two images gives the following result:</p>
<center>
<figure><img src="../inst/example_images/trees.jpg"><figcaption>
score: 0.89
</figcaption></figure><figure><img src="../inst/example_images/sky.jpg"><figcaption>
score: 0.42
</figcaption></figure>
</center>
</div>
<div class="section level3">
<h3 id="self-similarity">Self-similarity<a class="anchor" aria-label="anchor" href="#self-similarity"></a>
</h3>
<p>The function <code><a href="../reference/img_self_similarity.html">img_self_similarity()</a></code> returns the self-similarity of an image. Self-similarity can be measured with the Fourier power spectrum of an image. Previous research has identified that the spectral power of natural scenes falls with spatial frequencies (<span class="math inline">\(f\)</span>) according to a power law (<span class="math inline">\(\frac{1}{f^p}\)</span>) with values of <span class="math inline">\(p\)</span> near the value 2, which indicates scale invariance (for a review, see <span class="citation">[<a href="#ref-Simoncelli2001" role="doc-biblioref">14</a>]</span>). Therefore, the function computes self-similarity via the slope of the log-log power spectrum of the image using OLS.</p>
<p>The value for self-similarity that is returned by the function is calculated as <span class="math inline">\(\text{self-similarity} = |\text{slope} + 2| * (-1)\)</span>. That is, the measure reaches its maximum value of 0 for a slope of <span class="math inline">\(-2\)</span>, and any deviation from <span class="math inline">\(-2\)</span> results in negative values that are more negative the higher the deviation from <span class="math inline">\(-2\)</span>. Thus, the range of the self-similarity scores is <span class="math inline">\(-\infty\)</span> to <span class="math inline">\(0\)</span>. For color images, the weighted average between each color channel’s values is computed.</p>
<p>It is possible to get the raw regression slope (instead of the transformed value which indicates self-similarity) by using the option <code>raw = TRUE</code>. More options include the possibility to plot the log-log power spectrum (<code>logplot = TRUE</code>) and to base the computation of the slope on the full frequency spectrum (<code>full = TRUE</code>). See the function’s help file for details (i.e., <code><a href="../reference/img_self_similarity.html">?img_self_similarity</a></code>).</p>
<div class="sourceCode" id="cb8"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Example image with high self-similarity: romanesco</span></span>
<span><span class="va">romanesco</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/img_read.html">img_read</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/system.file.html" class="external-link">system.file</a></span><span class="op">(</span><span class="st">'example_images'</span>, <span class="st">'romanesco.jpg'</span>, package <span class="op">=</span> <span class="st">'imagefluency'</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="co"># display image</span></span>
<span><span class="fu">grid</span><span class="fu">::</span><span class="fu"><a href="https://rdrr.io/r/grid/grid.raster.html" class="external-link">grid.raster</a></span><span class="op">(</span><span class="va">romanesco</span><span class="op">)</span></span>
<span><span class="co"># get self-similarity</span></span>
<span><span class="fu"><a href="../reference/img_self_similarity.html">img_self_similarity</a></span><span class="op">(</span><span class="va">romanesco</span><span class="op">)</span></span></code></pre></div>
<div class="sourceCode" id="cb9"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Example image with low self-similarity: office</span></span>
<span><span class="va">office</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/img_read.html">img_read</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/system.file.html" class="external-link">system.file</a></span><span class="op">(</span><span class="st">'example_images'</span>, <span class="st">'office.jpg'</span>, package <span class="op">=</span> <span class="st">'imagefluency'</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="co"># display image</span></span>
<span><span class="fu">grid</span><span class="fu">::</span><span class="fu"><a href="https://rdrr.io/r/grid/grid.raster.html" class="external-link">grid.raster</a></span><span class="op">(</span><span class="va">office</span><span class="op">)</span></span>
<span><span class="co"># get self-similarity</span></span>
<span><span class="fu"><a href="../reference/img_self_similarity.html">img_self_similarity</a></span><span class="op">(</span><span class="va">office</span><span class="op">)</span></span></code></pre></div>
<p>Calculating the self-similarity scores for the two images gives the result below. The score for the romanesco broccoli is much closer to the maximum possible value of 0, hence much more self-similar.</p>
<center>
<figure><img src="inst/example_images/romanesco.jpg"><figcaption>
score: -0.03
</figcaption></figure><figure><img src="../inst/example_images/office.jpg"><figcaption>
score: -1.22
</figcaption></figure>
</center>
</div>
<div class="section level3">
<h3 id="symmetry">Symmetry<a class="anchor" aria-label="anchor" href="#symmetry"></a>
</h3>
<p>The function <code><a href="../reference/img_symmetry.html">img_symmetry()</a></code> returns the vertical and horizontal symmetry of an image as a numeric value between 0 (not symmetrical) and 1 (perfectly symmetrical).</p>
<p>Symmetry is computed as the correlation of corresponding image halves (i.e., the pairwise correlation of the corresponding pixels, cf. <span class="citation">[<a href="#ref-Mayer2014" role="doc-biblioref">15</a>]</span>). As the perceptual mirror axis is not necessarily exactly in the middle of a picture, the function detects in a first step the ‘optimal’ mirror axis by estimating several symmetry values with different positions for the mirror axis. To this end, the mirror axis is automatically shifted up to 5% and to the right (in the case of vertical symmetry; analogously for horizontal symmetry). In the second step, the overall symmetry score is computed as the maximum of the symmetry scores given the different mirror axes. For color images, the weighted average between each color channel’s values is computed. See <span class="citation">[<a href="#ref-MayerPACA2018" role="doc-biblioref">8</a>]</span> for details.</p>
<p>The function further has two optional logical parameters: <code>vertical</code> and <code>horizontal</code> (both <code>TRUE</code> by default). If one of the parameter is set to <code>FALSE</code>, the vertical or horizontal symmetry is not computed, respectively. See the function’s help file (i.e., <code><a href="../reference/img_symmetry.html">?img_symmetry</a></code>) for information about the additional options <code>shift_range</code> and <code>per_channel</code>.</p>
<div class="sourceCode" id="cb10"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Example image with high vertical symmetry: rails</span></span>
<span><span class="va">rails</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/img_read.html">img_read</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/system.file.html" class="external-link">system.file</a></span><span class="op">(</span><span class="st">'example_images'</span>, <span class="st">'rails.jpg'</span>, package <span class="op">=</span> <span class="st">'imagefluency'</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="co"># display image</span></span>
<span><span class="fu">grid</span><span class="fu">::</span><span class="fu"><a href="https://rdrr.io/r/grid/grid.raster.html" class="external-link">grid.raster</a></span><span class="op">(</span><span class="va">rails</span><span class="op">)</span></span>
<span><span class="co"># get only vertical symmetry</span></span>
<span><span class="fu"><a href="../reference/img_symmetry.html">img_symmetry</a></span><span class="op">(</span><span class="va">rails</span>, horizontal <span class="op">=</span> <span class="cn">FALSE</span><span class="op">)</span></span></code></pre></div>
<div class="sourceCode" id="cb11"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Example image with low vertical symmetry: bridge</span></span>
<span><span class="va">bridge</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/img_read.html">img_read</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/system.file.html" class="external-link">system.file</a></span><span class="op">(</span><span class="st">'example_images'</span>, <span class="st">'bridge.jpg'</span>, package <span class="op">=</span> <span class="st">'imagefluency'</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="co"># display image</span></span>
<span><span class="fu">grid</span><span class="fu">::</span><span class="fu"><a href="https://rdrr.io/r/grid/grid.raster.html" class="external-link">grid.raster</a></span><span class="op">(</span><span class="va">bridge</span><span class="op">)</span></span>
<span><span class="co"># get only vertical symmetry</span></span>
<span><span class="fu"><a href="../reference/img_symmetry.html">img_symmetry</a></span><span class="op">(</span><span class="va">bridge</span>, horizontal <span class="op">=</span> <span class="cn">FALSE</span><span class="op">)</span></span></code></pre></div>
<p>Calculating the vertical symmetry scores for the two images gives the following result:</p>
<center>
<figure><img src="inst/example_images/rails.jpg"><figcaption>
score: 0.96
</figcaption></figure><figure><img src="../inst/example_images/bridge.jpg"><figcaption>
score: 0.48
</figcaption></figure>
</center>
</div>
<div class="section level3">
<h3 id="image-typicality">Image typicality<a class="anchor" aria-label="anchor" href="#image-typicality"></a>
</h3>
<p>The function <code><a href="../reference/img_typicality.html">img_typicality()</a></code> returns the visual typicality of a <em>set</em> of images relative to each other. Values can range between -1 (inversely typical) over 0 (not typical) to 1 (perfectly typical). That is, higher absolute values indicate a larger typicality.</p>
<p>The typicality score is computed as the correlation of a particular image with the average representation of all images, i.e., the mean of all images <span class="citation">[<a href="#ref-Perrett1994" role="doc-biblioref">16</a>]</span>. That is, the typicality of an image can not be assessed in isolation, but only in comparison to set of images from the same category.</p>
<p>For color images, the weighted average between each color channel’s values is computed. If the images have different dimensions they are automatically resized to the smallest height and width. Rescaling of the images prior to computing the typicality scores can be specified with the optional rescaling parameter (must be a numeric value). With rescaling it is possible to assess typicality at different perceptual levels (see <span class="citation">[<a href="#ref-MayerDS2018" role="doc-biblioref">17</a>]</span> for details). Most users won’t need any rescaling and can use the default (<code>rescale = NULL</code>).</p>
<p>The following example shows three images of which two depict valleys in the mountains and one depicts fireworks. Therefore, the fireworks image is in comparison rather low in typicality in this set of images (i.e., atypical). It is important to note that an image’s typicality score highly depends on the reference set to which the image is compared to.</p>
<div class="sourceCode" id="cb12"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Example images depicting valleys: valley_white, valley_green</span></span>
<span><span class="co"># Example image depicting fireworks: fireworks</span></span>
<span><span class="va">valley_white</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/img_read.html">img_read</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/system.file.html" class="external-link">system.file</a></span><span class="op">(</span><span class="st">'example_images'</span>, <span class="st">'valley_white.jpg'</span>, package <span class="op">=</span> <span class="st">'imagefluency'</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="va">valley_green</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/img_read.html">img_read</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/system.file.html" class="external-link">system.file</a></span><span class="op">(</span><span class="st">'example_images'</span>, <span class="st">'valley_green.jpg'</span>, package <span class="op">=</span> <span class="st">'imagefluency'</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="va">fireworks</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/img_read.html">img_read</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/system.file.html" class="external-link">system.file</a></span><span class="op">(</span><span class="st">'example_images'</span>, <span class="st">'fireworks.jpg'</span>, package <span class="op">=</span> <span class="st">'imagefluency'</span><span class="op">)</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># create image set as list</span></span>
<span><span class="va">imglist</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/list.html" class="external-link">list</a></span><span class="op">(</span><span class="va">valley_white</span>, <span class="va">fireworks</span>, <span class="va">valley_green</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># get typicality</span></span>
<span><span class="fu"><a href="../reference/img_typicality.html">img_typicality</a></span><span class="op">(</span><span class="va">imglist</span><span class="op">)</span></span></code></pre></div>
<p>Calculating the typicality scores for the three images gives the following result:</p>
<center>
<figure><img src="../inst/example_images/valley_white.jpg"><figcaption>
score: 0.75
</figcaption></figure><figure><img src="inst/example_images/fireworks.jpg"><figcaption>
score: 0.40
</figcaption></figure><figure><img src="../../../../../../inst/example_images/valley_green.jpg"><figcaption>
score: 0.72
</figcaption></figure>
</center>
</div>
</div>
<div class="section level2">
<h2 id="summary">Summary<a class="anchor" aria-label="anchor" href="#summary"></a>
</h2>
<p><code>imagefluency</code> is an simple <code>R</code> package for image fluency scores. The package allows to get scores for several basic aesthetic principles that facilitate fluent cognitive processing of images. It straightforward to use and allows for an easy conversion of unstructured data into structured image features. These structured image features are naturally interpretable (i.e., no black-box-model). Finally, including such image information in statistical models might increase a model’s statistical and predictive power.</p>
</div>
<div class="section level2">
<h2 id="references">References<a class="anchor" aria-label="anchor" href="#references"></a>
</h2>
<small>
<div id="refs" class="references csl-bib-body" line-spacing="2">
<div id="ref-Hansen2008" class="csl-entry">
<div class="csl-left-margin">1. </div>
<div class="csl-right-inline">Hansen, J., Dechêne, A., &amp; Wänke, M. (2008). Discrepant fluency increases subjective truth. <em>Journal of Experimental Social Psychology</em>, <em>44</em>(3), 687–691. <a href="https://doi.org/10.1016/j.jesp.2007.04.005" class="external-link">https://doi.org/10.1016/j.jesp.2007.04.005</a>
</div>
</div>
<div id="ref-Reber2004" class="csl-entry">
<div class="csl-left-margin">2. </div>
<div class="csl-right-inline">Reber, R., Schwarz, N., &amp; Winkielman, P. (2004). Processing fluency and aesthetic pleasure: Is beauty in the perceiver’s processing experience? <em>Personality and Social Psychology Review</em>, <em>8</em>(4), 364–382. <a href="https://doi.org/10.1207/s15327957pspr0804_3" class="external-link">https://doi.org/10.1207/s15327957pspr0804_3</a>
</div>
</div>
<div id="ref-Schwarz2004" class="csl-entry">
<div class="csl-left-margin">3. </div>
<div class="csl-right-inline">Schwarz, N. (2004). Metacognitive experiences in consumer judgment and decision making. <em>Journal of Consumer Psychology</em>, <em>14</em>(4), 332–348. <a href="https://doi.org/10.1207/s15327663jcp1404_2" class="external-link">https://doi.org/10.1207/s15327663jcp1404_2</a>
</div>
</div>
<div id="ref-Graf2015" class="csl-entry">
<div class="csl-left-margin">4. </div>
<div class="csl-right-inline">Graf, L. K. M., &amp; Landwehr, J. R. (2015). A dual-process perspective on fluency-based aesthetics: The pleasure-interest model of aesthetic liking. <em>Personality and Social Psychology Review</em>, <em>19</em>(4), 395–410. <a href="https://doi.org/10.1177/1088868315574978" class="external-link">https://doi.org/10.1177/1088868315574978</a>
</div>
</div>
<div id="ref-Winkielman2001" class="csl-entry">
<div class="csl-left-margin">5. </div>
<div class="csl-right-inline">Winkielman, P., &amp; Cacioppo, J. T. (2001). Mind at ease puts a smile on the face: Psychophysiological evidence that processing facilitation elicits positive affect. <em>Journal of Personality and Social Psychology</em>, <em>81</em>(6), 989. <a href="https://doi.org/10.1037//0022-3514.81.6.989" class="external-link">https://doi.org/10.1037//0022-3514.81.6.989</a>
</div>
</div>
<div id="ref-Alter2009" class="csl-entry">
<div class="csl-left-margin">6. </div>
<div class="csl-right-inline">Alter, A. L., &amp; Oppenheimer, D. M. (2009). Uniting the tribes of fluency to form a metacognitive nation. <em>Personality and Social Psychology Review</em>, <em>13</em>(3), 219–235. <a href="https://doi.org/10.1177/1088868309341564" class="external-link">https://doi.org/10.1177/1088868309341564</a>
</div>
</div>
<div id="ref-Joye2016" class="csl-entry">
<div class="csl-left-margin">7. </div>
<div class="csl-right-inline">Joye, Y., Steg, L., Ünal, A. B., &amp; Pals, R. (2016). When complex is easy on the mind: Internal repetition of visual information in complex objects is a source of perceptual fluency. <em>Journal of Experimental Psychology: Human Perception and Performance</em>, <em>42</em>(1), 103–114. <a href="https://doi.org/10.1037/xhp0000105" class="external-link">https://doi.org/10.1037/xhp0000105</a>
</div>
</div>
<div id="ref-MayerPACA2018" class="csl-entry">
<div class="csl-left-margin">8. </div>
<div class="csl-right-inline">Mayer, S., &amp; Landwehr, J. R. (2018). Quantifying visual aesthetics based on processing fluency theory: Four algorithmic measures for antecedents of aesthetic preferences. <em>Psychology of Aesthetics, Creativity, and the Arts</em>, <em>12</em>(4), 399–431. <a href="https://doi.org/10.1037/aca0000187" class="external-link">https://doi.org/10.1037/aca0000187</a>
</div>
</div>
<div id="ref-Simoncelli2003" class="csl-entry">
<div class="csl-left-margin">9. </div>
<div class="csl-right-inline">Simoncelli, E. P. (2003). Vision and the statistics of the visual environment. <em>Current Opinion in Neurobiology</em>, <em>13</em>(2), 144–149. <a href="https://doi.org/10.1016/S0959-4388(03)00047-3" class="external-link">https://doi.org/10.1016/S0959-4388(03)00047-3</a>
</div>
</div>
<div id="ref-Peli1990" class="csl-entry">
<div class="csl-left-margin">10. </div>
<div class="csl-right-inline">Peli, E. (1990). Contrast in complex images. <em>Journal of the Optical Society of America A</em>, <em>7</em>(10), 2032–2040. <a href="https://doi.org/10.1364/JOSAA.7.002032" class="external-link">https://doi.org/10.1364/JOSAA.7.002032</a>
</div>
</div>
<div id="ref-Frazor2006" class="csl-entry">
<div class="csl-left-margin">11. </div>
<div class="csl-right-inline">Frazor, R. A., &amp; Geisler, W. S. (2006). Local luminance and contrast in natural images. <em>Vision Research</em>, <em>46</em>(10), 1585–1598. <a href="https://doi.org/10.1016/j.visres.2005.06.038" class="external-link">https://doi.org/10.1016/j.visres.2005.06.038</a>
</div>
</div>
<div id="ref-Donderi2006" class="csl-entry">
<div class="csl-left-margin">12. </div>
<div class="csl-right-inline">Donderi, D. C. (2006). Visual complexity: A review. <em>Psychological Bulletin</em>, <em>132</em>(1), 73–97. <a href="https://doi.org/10.1037/0033-2909.132.1.73" class="external-link">https://doi.org/10.1037/0033-2909.132.1.73</a>
</div>
</div>
<div id="ref-Landwehr2011" class="csl-entry">
<div class="csl-left-margin">13. </div>
<div class="csl-right-inline">Landwehr, J. R., Labroo, A. A., &amp; Herrmann, A. (2011). Gut liking for the ordinary: Incorporating design fluency improves automobile sales forecasts. <em>Marketing Science</em>, <em>30</em>(3), 416–429. <a href="https://doi.org/10.1287/mksc.1110.0633" class="external-link">https://doi.org/10.1287/mksc.1110.0633</a>
</div>
</div>
<div id="ref-Simoncelli2001" class="csl-entry">
<div class="csl-left-margin">14. </div>
<div class="csl-right-inline">Simoncelli, E. P., &amp; Olshausen, B. A. (2001). Natural image statistics and neural representation. <em>Annual Review of Neuroscience</em>, <em>24</em>(1), 1193–1216. <a href="https://doi.org/10.1146/annurev.neuro.24.1.1193" class="external-link">https://doi.org/10.1146/annurev.neuro.24.1.1193</a>
</div>
</div>
<div id="ref-Mayer2014" class="csl-entry">
<div class="csl-left-margin">15. </div>
<div class="csl-right-inline">Mayer, S., &amp; Landwehr, J. R. (2014). When complexity is symmetric: The interplay of two core determinants of visual aesthetics. In J. Cotte &amp; S. Wood (Eds.), <em><span class="nocase">Advances in Consumer Research</span></em> (Vol. 42, pp. 608–609). Duluth, MN: Association for Consumer Research.</div>
</div>
<div id="ref-Perrett1994" class="csl-entry">
<div class="csl-left-margin">16. </div>
<div class="csl-right-inline">Perrett, D. I., May, K. A., &amp; Yoshikawa, S. (1994). Facial shape and judgements of female attractiveness. <em>Nature</em>, <em>368</em>(6468), 239–242. <a href="https://doi.org/10.1038/368239a0" class="external-link">https://doi.org/10.1038/368239a0</a>
</div>
</div>
<div id="ref-MayerDS2018" class="csl-entry">
<div class="csl-left-margin">17. </div>
<div class="csl-right-inline">Mayer, S., &amp; Landwehr, J. R. (2018). Objective measures of design typicality. <em>Design Studies</em>, <em>54</em>, 146–161. <a href="https://doi.org/10.1016/j.destud.2017.09.004" class="external-link">https://doi.org/10.1016/j.destud.2017.09.004</a>
</div>
</div>
</div>
</small><p></p>
</div>
  </div>

  <div class="col-md-3 hidden-xs hidden-sm" id="pkgdown-sidebar">

        <nav id="toc" data-toggle="toc"><h2 data-toc-skip>Contents</h2>
    </nav>
</div>

</div>



      <footer><div class="copyright">
  <p></p>
<p>Developed by <a href="https://github.com/stm/" class="external-link">Stefan Mayer</a>.</p>
</div>

<div class="pkgdown">
  <p></p>
<p>Site built with <a href="https://pkgdown.r-lib.org/" class="external-link">pkgdown</a> 2.0.6.</p>
</div>

      </footer>
</div>

  


  

  </body>
</html>
